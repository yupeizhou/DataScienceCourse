{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#We assume data is in a parallel directory to this one called 'data'\n",
    "# cwd = os.getcwd()\n",
    "# datadir = '/'.join(cwd.split('/')[0:-1]) + '/data/'\n",
    "#or you can hardcode the directory\n",
    "datadir = '../data/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the data called survey_responses_2019.csv into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Student put in read data command here:\n",
    "data = pd.read_csv(os.path.join(datadir, 'survey_responses_2019.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column headers and use something more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cs_python', 'cs_java', 'cs_c', 'cs_perl', 'cs_javascript',\n",
       "       'cs_r', 'cs_sas', 'profile_1', 'profile_2', 'profile_3', 'profile_4',\n",
       "       'profile_5', 'profile_6', 'profile_7', 'len_answer', 'experience_coded',\n",
       "       'experience'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student put in code to look at column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names like 'profile_1-profile_7' aren't very descriptive. As a quick data maintenance task, let's rename the columns starting with 'profile'. The dictionary in the next cell maps the integer index to a descriptive text.\n",
    "\n",
    "Tactically, let's loop through each column name. Within the loop let's check whether the column name starts with 'profile.' If it does, let's create a new name that swaps the key with the value using profile_mapping dictionary (i.e., profile_1 -> profile_Viz). We then add the new column name to a list. If it doesn't start with 'profile' just add the old column name to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapping = {1:'Viz',\n",
    "                   2:'CS',\n",
    "                   3:'Math',\n",
    "                   4:'Stats',\n",
    "                   5:'ML',\n",
    "                   6:'Bus',\n",
    "                   7:'Com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Student put code here to change the header names\n",
    "newcols = []\n",
    "\n",
    "for colname in data.columns:\n",
    "    #finish the loop\n",
    "    if colname in ['profile_{}'.format(i) for i in range(8)]:\n",
    "        newcols.append('profile_{}'.format(profile_mapping[int(colname[-1])]))\n",
    "    else:\n",
    "        newcols.append(colname)\n",
    "    \n",
    "#Now swap the old columns with the values in newcols    \n",
    "data.columns = newcols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this data to illustrate common data analytic techniques. We have one numeric variable (len_answer) and different categorical variables which may carry some signal of the 'len_answer' variable. \n",
    "\n",
    "'Len_answer' is the character count of the response to the following question: \"Besides the examples given in lecture 1, discuss a case where data science has created value for some company. Please explain the company's goals and how any sort of data analysis could have helped the company achieve said goals.\" As this is a subjective business question, let's hypothesize that students with more professional experience might be more likely to give longer answers. \n",
    "\n",
    "In more technical terms, we'll test whether the variance of len_answer can be explained away by the categorical representation of a student's experience. \n",
    "\n",
    "The first thing we should do is look at the distribution of len_answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Student - plot a histogram here for len_answer\n",
    "plt.hist(data['len_answer'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have at least one strong outlier and a thick distribution around 0. Let's also use the Pandas describe() method to get a stronger sense of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     163.000000\n",
       "mean      523.478528\n",
       "std       348.918087\n",
       "min         0.000000\n",
       "25%       281.000000\n",
       "50%       471.000000\n",
       "75%       648.000000\n",
       "max      1901.000000\n",
       "Name: len_answer, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.len_answer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider cleaning up the data. We'll remove the top k values as well as those with a length less than 50 (which we think is a generous minimum to communicate a reasonable answer.\n",
    "\n",
    "Create a new data_frame that removes these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((155, 18), (163, 18))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a function to get the kth largest value of an array\n",
    "def get_kth_largest(inarray, k):\n",
    "    inarray.sort()\n",
    "    return inarray[-k]\n",
    "\n",
    "k = 3\n",
    "kth_largest = get_kth_largest(np.array(data.len_answer.values), 3)\n",
    "#Question = why did we wrap the series into an np.array() call in the above function call?\n",
    "\n",
    "#Student create a filtered data frame here\n",
    "data_clean = data[(data['len_answer'] < kth_largest) & (data['len_answer'] > 50)]\n",
    "\n",
    "#Compare the shape of both dataframes\n",
    "data_clean.shape, data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let's run a pairwise t-test on each experience level to see if their difference in len_answer is statistically significant. To run a t-test, we'll need the mean, standard-deviation and count for each group. We can achieve this with a pandas groupby operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">len_answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-5 years, I'm getting good at what I do!</th>\n",
       "      <td>601.965517</td>\n",
       "      <td>339.538498</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5+ years, I'm a veteran!</th>\n",
       "      <td>733.153846</td>\n",
       "      <td>403.083913</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt; 2 years, I'm fresh!</th>\n",
       "      <td>503.395349</td>\n",
       "      <td>284.094396</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None, I just finished my undergrad!</th>\n",
       "      <td>449.700000</td>\n",
       "      <td>249.228278</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           len_answer                  \n",
       "                                                 mean         std count\n",
       "experience                                                             \n",
       "2-5 years, I'm getting good at what I do!  601.965517  339.538498    29\n",
       "5+ years, I'm a veteran!                   733.153846  403.083913    13\n",
       "< 2 years, I'm fresh!                      503.395349  284.094396    43\n",
       "None, I just finished my undergrad!        449.700000  249.228278    70"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student input code here\n",
    "data_clean_grouped = data_clean[['experience', 'len_answer']].groupby(['experience']).agg(['mean', 'std', 'count'])\n",
    "#run this to look at the grouped df\n",
    "data_clean_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see a potential split between the [0, 2] year experience range and the [2+] experience range. Let's be more rigorous and run t-tests. Let's write a function that takes in the necessary statistics and returns a p-value.\n",
    "\n",
    "Remember, the t-stat for the difference between two means is:\n",
    "\n",
    "<center>$t = \\frac{\\hat{\\mu_1} - \\hat{\\mu_2}}{\\sqrt{\\frac{\\hat{\\sigma_1}^2}{n_1} + \\frac{\\hat{\\sigma_2}^2}{n_2}}}$</center>\n",
    "\n",
    "The p-value can be found using a t-distribution, but for simplicity, let's approximate this with the normal distribution. For the 2-tailed test, the p-value is: 2 * (1 - Norm.CDF(T))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Student complete the function\n",
    "from scipy.stats import norm\n",
    "def pvalue_diffmeans_twotail(mu1, sig1, n1, mu2, sig2, n2):\n",
    "    '''\n",
    "    P-value calculator for the hypothesis test of mu1 != mu2.\n",
    "    Takes in the approprate inputs to compute the t-statistic for the difference between means\n",
    "    Outputs a p-value for a two-sample t-test.\n",
    "    '''\n",
    "    t = (mu1 - mu2) / np.sqrt(sig1 ** 2 / n1 + sig2 ** 2 / n2)\n",
    "    p_value = 2 * (1 - norm.cdf(abs(t)))\n",
    "    \n",
    "    return (t, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through all possible pairs in data_clean_grouped and perform a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and 5+ years, I'm a veteran!\n",
      "Diff = -131.0 characters\n",
      "The t-stat is -1.022 and p-value is 0.307\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and < 2 years, I'm fresh!\n",
      "Diff = 99.0 characters\n",
      "The t-stat is 1.288 and p-value is 0.198\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and None, I just finished my undergrad!\n",
      "Diff = 152.0 characters\n",
      "The t-stat is 2.184 and p-value is 0.029\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and < 2 years, I'm fresh!\n",
      "Diff = 230.0 characters\n",
      "The t-stat is 1.916 and p-value is 0.055\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and None, I just finished my undergrad!\n",
      "Diff = 283.0 characters\n",
      "The t-stat is 2.45 and p-value is 0.014\n",
      "\n",
      "Two tailed T-Test between groups: < 2 years, I'm fresh! and None, I just finished my undergrad!\n",
      "Diff = 54.0 characters\n",
      "The t-stat is 1.021 and p-value is 0.307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Student put in code here:\n",
    "\n",
    "#get distinct values in the data frame for the experience variable\n",
    "grps = list(data_clean_grouped.index)  \n",
    "\n",
    "#Now loop through each pair\n",
    "for i, grp1 in enumerate(grps):\n",
    "    for grp2 in grps[i + 1:]:\n",
    "    \n",
    "        '''\n",
    "        Also, the result of groupby uses a multi-index. So be sure to index on 'len_answer' as well.\n",
    "        Then pull out the mean, std, and cnt from that result.   \n",
    "        ''' \n",
    "        row1 = data_clean_grouped.loc[grp1].loc['len_answer']\n",
    "        row2 = data_clean_grouped.loc[grp2].loc['len_answer']\n",
    "        \n",
    "        tstat, p_value = pvalue_diffmeans_twotail(row1['mean'], row1['std'], row1['count'], \n",
    "                                                  row2['mean'], row2['std'], row2['count']) \n",
    "\n",
    "        #some code should go here\n",
    "        \n",
    "        print('Two tailed T-Test between groups: {} and {}'.format(grp1, grp2))\n",
    "        print('Diff = {} characters'.format(round(row1['mean'] - row2['mean'], 0)))\n",
    "        print('The t-stat is {} and p-value is {}'.format(round(tstat, 3), round(p_value, 3)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some observations you might have about the above results? Are there any with large deviances that are not statistically significant at at least a 95% level? Is there any issue with using 95% as our threshold for statistical significance? In fact there is. We are running multiple hypothesis tests at once, and doing this is known to increase the probability that we have at least one false positive (i.e., $P(False Positive) = 1 - .95^{Ntests}$). We can apply a simplye but conservative method called the <a href=\"https://en.wikipedia.org/wiki/Bonferroni_correction\">Bonferoni Correction</a>, which says that if we normally would care about an alpha level of $\\alpha$ for significance testing, and we're doing $N$ tests, then our new significance level should be $\\alpha/N$. This correction is conservative because it assumes that each test is independent. Since each group is repeatedly sampled across pairs, we know that our individual tests are not indeed independent. Nonetheless, we'll see how the results hold under this new regime. \n",
    "\n",
    "Also, how do the numbers change if you rerun it using the original data, and not the cleaned data. What is the effect of outliers on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and 5+ years, I'm a veteran!\n",
      "Diff = -75.0 characters\n",
      "The t-stat is -0.548 and p-value is 0.584\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and < 2 years, I'm fresh!\n",
      "Diff = 176.0 characters\n",
      "The t-stat is 1.925 and p-value is 0.054\n",
      "\n",
      "Two tailed T-Test between groups: 2-5 years, I'm getting good at what I do! and None, I just finished my undergrad!\n",
      "Diff = 205.0 characters\n",
      "The t-stat is 2.357 and p-value is 0.018\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and < 2 years, I'm fresh!\n",
      "Diff = 251.0 characters\n",
      "The t-stat is 2.092 and p-value is 0.036\n",
      "\n",
      "Two tailed T-Test between groups: 5+ years, I'm a veteran! and None, I just finished my undergrad!\n",
      "Diff = 280.0 characters\n",
      "The t-stat is 2.4 and p-value is 0.016\n",
      "\n",
      "Two tailed T-Test between groups: < 2 years, I'm fresh! and None, I just finished my undergrad!\n",
      "Diff = 29.0 characters\n",
      "The t-stat is 0.522 and p-value is 0.602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rerun everything without cleaning outliers\n",
    "\n",
    "data_grouped = data[['experience', 'len_answer']].groupby(['experience']).agg(['mean', 'std', 'count'])  \n",
    "grps = list(data_grouped.index) \n",
    "\n",
    "#Now loop through each pair\n",
    "for i, grp1 in enumerate(grps):\n",
    "    for grp2 in grps[i + 1:]:\n",
    "    \n",
    "        '''\n",
    "        Also, the result of groupby uses a multi-index. So be sure to index on 'len_answer' as well.\n",
    "        Then pull out the mean, std, and cnt from that result.   \n",
    "        '''\n",
    "        \n",
    "        row1 = data_grouped.loc[grp1].loc['len_answer']\n",
    "        row2 = data_grouped.loc[grp2].loc['len_answer']\n",
    "        \n",
    "        tstat, p_value = pvalue_diffmeans_twotail(row1['mean'], row1['std'], row1['count'], \n",
    "                                                  row2['mean'], row2['std'], row2['count']) \n",
    "    \n",
    "        print('Two tailed T-Test between groups: {} and {}'.format(grp1, grp2))\n",
    "        print('Diff = {} characters'.format(round(row1['mean'] - row2['mean'], 0)))\n",
    "        print('The t-stat is {} and p-value is {}'.format(round(tstat, 3), round(p_value, 3)))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
